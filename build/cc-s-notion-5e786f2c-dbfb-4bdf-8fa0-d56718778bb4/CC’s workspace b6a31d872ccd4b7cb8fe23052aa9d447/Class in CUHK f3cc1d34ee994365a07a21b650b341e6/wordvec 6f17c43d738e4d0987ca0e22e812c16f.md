# wordvec

Class: CS224n
Created: September 18, 2022 7:29 PM
Finished: No
Materials: Lecture01-wordvecs-note.pdf, Lecture01-wordvecs.pdf, lecture02-wordvecs2.pdf

---

- [ ]  the deviation part of centre word and context word is real confusing

---

- [ ]  what is convex
    
    ![Untitled](wordvec%206f17c43d738e4d0987ca0e22e812c16f/Untitled.png)
    

---

- [ ]  SGD的sample怎么解释？

---

- [ ]  What is negative sampling?    Additional effiency in training

---

- [ ]  How to select negative sample?

---

- [x]  why use 2 vectors to represent word?
    - it is not likely that a word appears in its own context, so you would want to minimize the probability p(w|w). But if you use the same vectors for w as context than for w as center word, you cannot minimize p(w|w) (computed via the dot product) if you are to keep word embeddings in the unit circle.

---

- [x]  why is softmax “softmax”
    
    ![Untitled](wordvec%206f17c43d738e4d0987ca0e22e812c16f/Untitled%201.png)
    
    - softmax function maps arbitrary values xi to a probability diistribution pi
    - max: amplifies probability of largest xi
    - soft: still assign probability to smaller xi
    - what do softmax do in DL?
        - If we add a softmax layer to the network, it is possible to translate the numbers into a probability distribution.

---

- [x]  Objective function in word2vec
    - Data likelihood:
        
        For each position 𝑡 = 1, ... , 𝑇, predict context words within a window of fixed size *m*, given center word 𝑤t. 
        
        ![Untitled](wordvec%206f17c43d738e4d0987ca0e22e812c16f/Untitled%202.png)
        
    - objective function 𝐽 (𝜃): (average) negative log likelihood:
        
        ![Untitled](wordvec%206f17c43d738e4d0987ca0e22e812c16f/Untitled%203.png)
        
        - Minimizing objective function ⟺ Maximizing predictive accuracy

---

- [ ]  how to minimize the objective function? = how to minimize loss? = how to train the model?
    - the process of training a model is the process of adjusting parameters to minimize loss
    - compute all vector gradients to optimize (by walking down the gradient)
    - ****Word2vec derivations of gradient****
        - for one example window and one example outside word:
            
            ![Untitled](wordvec%206f17c43d738e4d0987ca0e22e812c16f/Untitled%204.png)
            

---

- [x]  如下图紫色问题

![Untitled](wordvec%206f17c43d738e4d0987ca0e22e812c16f/Untitled%205.png)

![Untitled](wordvec%206f17c43d738e4d0987ca0e22e812c16f/Untitled%206.png)

![Untitled](wordvec%206f17c43d738e4d0987ca0e22e812c16f/Untitled%207.png)

---

- [x]  what parameters do we adjust during training？what is θ？
- 𝜃 represents **all** the model parameters, in one long vector

![Untitled](wordvec%206f17c43d738e4d0987ca0e22e812c16f/Untitled%208.png)

- every variable in θ is a d dimensional vector
- there is V lemmas in the vocabulary
- so the total number of parameters is 2dV

---